"""regression:

"""

from __future__ import division

import os
import os.path as path

from itertools import count
import time

import numpy as np

from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import MinMaxScaler

import lasagne
from lasagne.layers import InputLayer, DenseLayer, DropoutLayer

import theano
import theano.tensor as T

def testitems_sort(X1_test, X2_test, Y1_test, Y2_test, test_info):
    #Sorts testsets into subsets for ADS and IDS, and congruent and incongruent items
    X_test_ADS_c = []
    X_test_ADS_i = []
    X_test_IDS_c = []
    X_test_IDS_i = []
    Y_test_ADS_c = []
    Y_test_ADS_i = []
    Y_test_IDS_c = []
    Y_test_IDS_i = []

    for i in range(len(test_info)):
        if test_info[i][1] == 'ADS':    #ADS setting
            if test_info[i][0] == '1':   #first item is congruent
                X_test_ADS_c.append(X1_test[i])
                X_test_ADS_i.append(X2_test[i])
                Y_test_ADS_c.append(Y1_test[i])
                Y_test_ADS_i.append(Y2_test[i])
            else:                       #second item is congruent
                X_test_ADS_c.append(X2_test[i])
                X_test_ADS_i.append(X1_test[i])
                Y_test_ADS_c.append(Y2_test[i])
                Y_test_ADS_i.append(Y1_test[i])
        else:                            #IDS setting
            if test_info[i][0] == '1':   #first item is congruent
                X_test_IDS_c.append(X1_test[i])
                X_test_IDS_i.append(X2_test[i])
                Y_test_IDS_c.append(Y1_test[i])
                Y_test_IDS_i.append(Y2_test[i])
            else:                       #second item is congruent
                X_test_IDS_c.append(X2_test[i])
                X_test_IDS_i.append(X1_test[i])
                Y_test_IDS_c.append(Y2_test[i])
                Y_test_IDS_i.append(Y1_test[i])
    return dict(
        X_ADS_c = X_test_ADS_c,
        X_ADS_i = X_test_ADS_i,
        X_IDS_c = X_test_IDS_c,
        X_IDS_i = X_test_IDS_i,
        Y_ADS_c = Y_test_ADS_c,
        Y_ADS_i = Y_test_ADS_i,
        Y_IDS_c = Y_test_IDS_c,
        Y_IDS_i = Y_test_IDS_i
    )

def make_data(dir_train, dir_test, condition, debug, method, n_samples=1000, n_features=1, n_targets=1, informative_prop=1.0,
              noise=0.0, valid_prop=0.3):
#Read in data generated by bottleneck_features.py
    if method == 'bnf1':
    	train_file = dir_train + 'train_condition' + str(condition) + 'model1.npz'
    	test_file = dir_test + 'test_condition' + str(condition) + 'model1.npz'
        train_dataset = np.load(train_file)
        test_dataset = np.load(test_file)
    elif method == 'bnf2':
    	train_file = dir_train + 'train_condition' + str(condition) + 'model2.npz'
    	test_file = dir_test + 'test_condition' + str(condition) + 'model2.npz'
    	train_dataset = np.load(train_file)
        test_dataset = np.load(test_file)    
    else:
    	raise ValueError('model unknown')

    train_X, train_y, train_labels, train_info = train_dataset['X'], train_dataset['y'], train_dataset['labels'], train_dataset['info']
    test_X1, test_X2, test_y1, test_y2, test_labels, test_info = test_dataset['X1'], test_dataset['X2'],test_dataset['y1'], test_dataset['y2'], test_dataset['labels'], test_dataset['info']

    print type(train_X)

    X_train = MinMaxScaler(feature_range=(0.0,1.0)).fit_transform(train_X)
    X_train = X_train.astype(theano.config.floatX)
    Y_train = MinMaxScaler(feature_range=(0.0,1.0)).fit_transform(train_y)
    Y_train = Y_train.astype(theano.config.floatX)

    X_test = MinMaxScaler(feature_range=(0.0,1.0)).fit_transform(X_test)
    X_test = X_test.astype(theano.config.floatX)
    Y_test = MinMaxScaler(feature_range=(0.0,1.0)).fit_transform(Y_test)
    Y_test= Y_test.astype(theano.config.floatX)
    X_test = MinMaxScaler(feature_range=(0.0,1.0)).fit_transform(X_test)
    X_test = X_test.astype(theano.config.floatX)
    Y_test = MinMaxScaler(feature_range=(0.0,1.0)).fit_transform(Y_test)
    Y_test= Y_test.astype(theano.config.floatX)

    if len(X_train.shape) > 1:
        n_features = X_train.shape[1]
    else:
        X_train = X_train.reshape(X_train.shape[0], -1)
        n_features = 1
    if len(Y_train.shape) > 1:
        n_targets = Y_train.shape[1]
    else:
        Y_train = Y_train.reshape(Y_train.shape[0], -1)
        n_targets = 1

    X_train, Y_train, X_valid, Y_valid = \
        train_valid_split(X_train, Y_train,
                               test_prop=valid_prop, valid_prop=valid_prop)
    X_ADS_c, X_ADS_i, X_IDS_c, X_IDS_i, Y_ADS_c, Y_ADS_i, Y_IDS_c, Y_IDS_i = \
        Y_test_IDS_i= testitems_sort(X1_test, X2_test, Y1_test, Y2_test, test_info)

    return dict(
        X_train=theano.shared(X_train),
        Y_train=theano.shared(Y_train),
        X_valid=theano.shared(X_valid),
        Y_valid=theano.shared(Y_valid),
        X_ADS_c=theano.shared(X_ADS_c),
        X_ADS_i=theano.shared(X_ADS_i),
        X_IDS_c=theano.shared(X_IDS_c),
        X_IDS_i=theano.shared(X_IDS_i),
        Y_ADS_c=theano.shared(Y_ADS_c),
        Y_ADS_i=theano.shared(Y_ADS_i),
        Y_IDS_c=theano.shared(Y_IDS_c),
        Y_IDS_i=theano.shared(Y_IDS_i),
        num_examples_train=X_train.shape[0], 
        num_examples_valid=X_valid.shape[0],
        num_examples_test= 2*X1_test.shape[0],
        input_dim=n_features,
        output_dim=n_targets)

def train_valid_split(X, y, valid_prop=0.2):
    nsamples = X.shape[0]
    ixs = np.random.permutation(nsamples)
    X = np.copy(X)
    X = X[ixs]
    y = np.copy(y)
    y = y[ixs]
    valid_cut = int(valid_prop*nsamples)
    X_valid, y_valid = X[:valid_cut], y[:valid_cut]
    X_train, y_train = X[valid_cut:], y[valid_cut:]
    return X_train, y_train, X_valid, y_valid

def build_model(input_dim, output_dim,
                hidden_layers=(100, 100, 100),
                batch_size=100, dropout=True):
    l_in = InputLayer(shape=(batch_size, input_dim))
    last = l_in
    for size in hidden_layers[:-1]:
        l_hidden = DenseLayer(last, num_units=size,
                              nonlinearity=lasagne.nonlinearities.leaky_rectify,
                              W=lasagne.init.GlorotUniform())
        if dropout:
            l_dropout = DropoutLayer(l_hidden, p=0.5)
            last = l_dropout
        else:
            last = l_hidden
    l_penult = DenseLayer(last, num_units=hidden_layers[-1],
                          nonlinearity=lasagne.nonlinearities.leaky_rectify,
                          W=lasagne.init.GlorotUniform())
    l_out = DenseLayer(l_penult, num_units=output_dim,
                       nonlinearity=lasagne.nonlinearities.linear)
    return l_out

def create_iter_funcs(dataset, output_layer,
                      tensor_type=T.matrix,
                      batch_size=300,
                      learning_rate=0.01,
                      momentum=0.9):
    batch_index = T.iscalar('batch_index')
    X_batch = tensor_type('x')
    Y_batch = tensor_type('y')
    batch_slice = slice(batch_index * batch_size,
                        (batch_index + 1) * batch_size)

    objective = lasagne.objectives.Objective(output_layer,
        loss_function=lasagne.objectives.mse)
    loss_train = objective.get_loss(X_batch, target=Y_batch)
    loss_eval = objective.get_loss(X_batch, target=Y_batch,
                                   deterministic=True)

    all_params = lasagne.layers.get_all_params(output_layer)
    updates = lasagne.updates.sgd(
        loss_or_grads=loss_train,
        params=all_params,
        learning_rate=learning_rate)

    iter_train = theano.function(
        [batch_index], loss_train,
        updates=updates,
        givens={
            X_batch: dataset['X_train'][batch_slice],
            Y_batch: dataset['Y_train'][batch_slice],
        }
    )

    iter_valid = theano.function(
        [batch_index], loss_eval,
        givens={
            X_batch: dataset['X_valid'][batch_slice],
            Y_batch: dataset['Y_valid'][batch_slice],
        }
    )
	
    return dict(
        train=iter_train,
        valid=iter_valid)

def train(iter_funcs, dataset, batch_size=300):
    num_batches_train = dataset['num_examples_train'] // batch_size
    num_batches_valid = dataset['num_examples_valid'] // batch_size

    for epoch in count(1):
        batch_train_losses = []
        for b in xrange(num_batches_train):
            batch_train_loss = iter_funcs['train'](b)
            batch_train_losses.append(batch_train_loss)

        avg_train_loss = np.mean(batch_train_losses)

        batch_valid_losses = []
        for b in xrange(num_batches_valid):
            batch_valid_loss = iter_funcs['valid'](b)
            batch_valid_losses.append(batch_valid_loss)

        avg_valid_loss = np.mean(batch_valid_losses)

        yield {
            'number': epoch,
            'train_loss': avg_train_loss,
            'valid_loss': avg_valid_loss
        }

if __name__ == '__main__':
    num_epochs = 10000
    batch_size = 1000 
    dir_train = '/Users/Research/projects/phonrulemodel/bnftrainsets/'
    dir_test = '/Users/Research/projects/phonrulemodel/bnftestsets/'
    dir_output = '/Users/Research/projects/phonrulemodel/regressionoutput/'
    #For bnf's generated with model 1: method = bnf1, generated with model 2: method = bnf2
    #Condition: experimental condition (1-8), if debug = True, use subset of data for debugging
    #without GPU
    condition = 1
    debug = True
    method = 'bnf1'
    dataset = make_data(dir_train, dir_test, condition, debug, method, n_features=10, n_targets=10)
    output_layer = build_model(
        input_dim=dataset['input_dim'], output_dim=dataset['output_dim'],
        batch_size=batch_size)
    iter_funcs = create_iter_funcs(dataset, output_layer,
                                   batch_size=batch_size,
                                   learning_rate=0.1, momentum=0.9)
    
    # Exposure phase
    now = time.time()
    try:
        for epoch in train(iter_funcs, dataset,
                           batch_size=batch_size):
            print('Epoch {} of {} took {:.3f}s'.format(
                epoch['number'], num_epochs, time.time() - now))
            now = time.time()
            print("  training loss:\t\t{:.6f}".format(epoch['train_loss']))
            print("  validation loss:\t\t{:.6f}".format(epoch['valid_loss']))
            if epoch['number'] >= num_epochs:
                break
    except KeyboardInterrupt:
        pass

    # Test phase
    X_ADS_c= dataset['X_ADS_c'].get_value()
    X_ADS_i= dataset['X_ADS_i'].get_value()
    X_IDS_c= dataset['X_IDS_c'].get_value()
    X_IDS_i= dataset['X_IDS_i'].get_value()
        

    slices = [slice(batch_index*batch_size, (batch_index+1)*batch_size)
              for batch_index in xrange(X1_test.shape[0] // batch_size)]
    Y_ADS_c_pred = np.vstack((output_layer.get_output(X_ADS_c[sl]).eval()
                        for sl in slices))
    Y_ADS_i_pred = np.vstack((output_layer.get_output(X_ADS_i[sl]).eval()
                        for sl in slices))
    Y_IDS_c_pred = np.vstack((output_layer.get_output(X_IDS_c[sl]).eval()
                        for sl in slices))
    Y_IDS_i_pred = np.vstack((output_layer.get_output(X_IDS_i[sl]).eval()
                        for sl in slices))
    # print('Y_pred')
    # print(Y_pred)
    # print(Y_pred.shape)

    Y_ADS_c= dataset['Y_ADS_c'].get_value()
    Y_ADS_i= dataset['Y_ADS_i'].get_value()
    Y_IDS_c= dataset['Y_IDS_c'].get_value()
    Y_IDS_i= dataset['Y_IDS_i'].get_value()

    # print('Y_test')
    # print(Y_test)
    # print(Y_test.shape)

    # expl_var = explained_variance_score(Y_test, Y_pred)
    mse_ADS_c = mean_squared_error(Y_ADS_c, Y_ADS_c_pred)
    mse_ADS_i = mean_squared_error(Y_ADS_i, Y_ADS_i_pred)
    mse_IDS_c = mean_squared_error(Y_IDS_c, Y_IDS_c_pred)
    mse_IDS_i = mean_squared_error(Y_IDS_i, Y_IDS_i_pred)

    # print( 'Explained variance: {0:.3f}'.format(expl_var))
    print( 'Mean squared error 1: {0:.3f}'.format(mse_ADS_c))
    print( 'Mean squared error 2: {0:.3f}'.format(mse_ADS_i))
    print( 'Mean squared error 1: {0:.3f}'.format(mse_IDS_c))
    print( 'Mean squared error 2: {0:.3f}'.format(mse_IDS_i))

    #Make comparison between IDS stimuli and ADS stimuli

    #Save results
    output = dir_output + 'results_condition' + condition + '_model' + method
    np.savez(output, mse_ADS_c = mse_ADS_c, mse_ADS_i = mse_ADS_i, mse_IDS_c = mse_IDS_c, mse_IDS_i = mse_IDS_i)

# Y_pred = net.predict(X_test)
# expl_var = explained_variance_score(Y_test, Y_pred)
# mse = mean_squared_error(Y_test, Y_pred)
# r2 = r2_score(Y_test, Y_pred)

# print 'Timing:'
# print 'Training time: {0:.3f}s'.format(train_time)
# print 'Test time:     {0:.3f}s'.format(test_time)
# print '---'

# print 'Test performance:'